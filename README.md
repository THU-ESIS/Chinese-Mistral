<div align="center">
    <h1>
        Chinese-Mistral
    </h1>
</div>

<p align="center">
    <br>
    <img src="img/logo.png" width="600"/>
    <br>
</p>

<div align="center">
    <a href="https://github.com/THU-ESIS/Chinese-Mistral/pulls">
        <image src="https://img.shields.io/badge/PRs-welcome-brightgreen"></image>
        <image src="https://img.shields.io/badge/License-Apache_2.0-green.svg"></image>
    </a>
</div>

## ğŸ‰ æ–°é—»
- [2024-10-11] [æ–°æ–‡é€Ÿé€’|PreparedLLMï¼šé«˜æ•ˆè®­ç»ƒé¢†åŸŸå¤§è¯­è¨€æ¨¡å‹çš„â€œå‰é¢„è®­ç»ƒâ€æ¡†æ¶](https://mp.weixin.qq.com/s/ugJQ9tbp6Y87xA3TOWteqw)ã€‚
- [2024-08-31] æ–‡ç« [PreparedLLM: Effective Pre-pretraining Framework for Domain-specific Large Language Models](https://www.tandfonline.com/doi/full/10.1080/20964471.2024.2396159)å·²è¢«*Big Earth Data*æœŸåˆŠæ¥æ”¶ã€‚
- [2024-08-31] å‘å¸ƒ[Chinese-Mistral-7B-Instruct-v0.2](https://huggingface.co/itpossible/Chinese-Mistral-7B-Instruct-v0.2)å¯¹è¯æ¨¡å‹ã€‚è¯­è¨€ç†è§£èƒ½åŠ›å¤§å¹…æé«˜ï¼Œå¹¶ä¸”å…·å¤‡å¤šè½®å¯¹è¯èƒ½åŠ›ã€‚
- [2024-06-30] å‘å¸ƒ[JiuZhou-Instruct-v0.2](https://huggingface.co/itpossible/JiuZhou-Instruct-v0.2)å¯¹è¯æ¨¡å‹ã€‚è¯­è¨€ç†è§£èƒ½åŠ›å¤§å¹…æé«˜ï¼Œå¹¶ä¸”å…·å¤‡å¤šè½®å¯¹è¯èƒ½åŠ›ã€‚
- [2024-04-04] å‘å¸ƒ[Chinese-Mistral-7B-Instruct-v0.1](https://huggingface.co/itpossible/Chinese-Mistral-7B-Instruct-v0.1)ã€‚
- [2024-03-31] å‘å¸ƒ[Chinese-Mistral-7B-v0.1](https://huggingface.co/itpossible/Chinese-Mistral-7B)åŸºåº§æ¨¡å‹ã€‚

## ğŸš€ ä»‹ç»

éšç€Mistral AIå…¬å¸å¼€æºå…¶ä¸ƒåäº¿å‚æ•°æ¨¡å‹[Mistral-7B](https://huggingface.co/meta-llama/Llama-2-7b-hf)ï¼Œè¯¥æ¨¡å‹è¶…è¶Š[Llama](https://huggingface.co/meta-llama)ï¼Œæˆä¸ºå½“å‰æœ€å¼ºå¤§çš„å¼€æºæ¨¡å‹ä¹‹ä¸€ã€‚Mistral-7Båœ¨å„ç±»åŸºå‡†æµ‹è¯•ä¸­ï¼Œä¸ä»…è¶…è¿‡äº†Llama2-13Bï¼Œè€Œä¸”åœ¨æ¨ç†ã€æ•°å­¦ã€ä»£ç ç”Ÿæˆä»»åŠ¡ä¸­è¶…è¿‡Llama2-34Bã€‚
ç„¶è€Œï¼ŒMistral-7Bçš„è®­ç»ƒè¯­æ–™ä¸»è¦ä¸ºè‹±æ–‡æ–‡æœ¬ï¼Œå…¶ä¸­æ–‡èƒ½åŠ›è¾ƒä¸ºæ¬ ç¼ºã€‚å…¶æ¬¡ï¼ŒMistral-7Bçš„è¯è¡¨ä¸æ”¯æŒä¸­æ–‡ï¼Œå¯¼è‡´å…¶å¯¹ä¸­æ–‡çš„ç¼–ç å’Œè§£ç æ•ˆç‡è¾ƒä½ï¼Œé™åˆ¶äº†åœ¨ä¸­æ–‡åœºæ™¯çš„åº”ç”¨ã€‚<br>
ä¸ºäº†å…‹æœè¿™ä¸€å±€é™ï¼Œæ¸…åå¤§å­¦åœ°çƒç³»ç»Ÿç§‘å­¦ç³»åœ°çƒå’Œç©ºé—´ä¿¡æ¯ç§‘å­¦å®éªŒå®¤åŸºäºMistral-7Bè¿›è¡Œäº†ä¸­æ–‡è¯è¡¨æ‰©å……å’Œå¢é‡é¢„è®­ç»ƒï¼Œå¢å¼ºäº†Mistral-7Båœ¨ä¸­æ–‡ä»»åŠ¡ä¸Šçš„è¡¨ç°ï¼Œå¹¶æé«˜äº†å…¶å¯¹ä¸­æ–‡æ–‡æœ¬çš„ç¼–è§£ç æ•ˆç‡ã€‚

## ğŸ“¥ æ¨¡å‹ä¸‹è½½

æœ¬é¡¹ç›®å¼€æºäº†Chinese-Mistral-7Bä¸Chinese-Mistral-7B-Instructï¼š

|             æ¨¡å‹             |                                     ä¸‹è½½åœ°å€                                      |                                                         è¯´æ˜                                                          |
|:-----------------------------:|:------------------------------------------------------------------------------:|:-------------------------------------------------------------------------------------------------------------------:|
|     Chinese-Mistral-7B     |     [HuggingFace](https://huggingface.co/itpossible/Chinese-Mistral-7B-v0.1)<br>[wisemodel](https://wisemodel.cn/models/itpossible/Chinese-Mistral-7B-v0.1)<br>[ModelScope](https://www.modelscope.cn/models/itpossible/Chinese-Mistral-7B-v0.1)     |                                                  å®Œæ•´åŸºåº§æ¨¡å‹                                                  |
| Chinese-Mistral-7B-Instruct-v0.1 | [HuggingFace](https://huggingface.co/itpossible/Chinese-Mistral-7B-Instruct-v0.1)<br>[wisemodel](https://wisemodel.cn/models/itpossible/Chinese-Mistral-7B-Instruct-v0.1)<br>[ModelScope](https://www.modelscope.cn/models/itpossible/Chinese-Mistral-7B-Instruct-v0.1) | å®Œæ•´æŒ‡ä»¤ç²¾è°ƒæ¨¡å‹<br>ä¸­è‹±æ–‡alpaca_gpt4è¿›è¡Œloraå¾®è°ƒ|
| Chinese-Mistral-7B-Instruct-v0.2 | [HuggingFace](https://huggingface.co/itpossible/Chinese-Mistral-7B-Instruct-v0.2)<br>[wisemodel](https://wisemodel.cn/models/itpossible/Chinese-Mistral-7B-Instruct-v0.2)<br> | å®Œæ•´æŒ‡ä»¤ç²¾è°ƒæ¨¡å‹<br>ç™¾ä¸‡æ¡é«˜è´¨é‡æ•°æ®è¿›è¡Œloraå¾®è°ƒ|


## ğŸ’» æ¨¡å‹æ¨ç†

å¦‚ä¸‹æ˜¯ä½¿ç”¨Chinese-Mistral-7Bè¿›è¡Œæ¨ç†çš„ä»£ç ç¤ºä¾‹ã€‚

```python
import torch
from transformers import AutoTokenizer, AutoModelForCausalLM

device = torch.device("cuda:0") if torch.cuda.is_available() else torch.device("cpu")

model_path = "itpossible/Chinese-Mistral-7B-v0.1"
tokenizer = AutoTokenizer.from_pretrained(model_path)
model = AutoModelForCausalLM.from_pretrained(model_path, torch_dtype=torch.bfloat16, device_map=device)

text = "æˆ‘æ˜¯ä¸€ä¸ªäººå·¥æ™ºèƒ½åŠ©æ‰‹ï¼Œæˆ‘èƒ½å¤Ÿå¸®åŠ©ä½ åšå¦‚ä¸‹è¿™äº›äº‹æƒ…ï¼š"
inputs = tokenizer(text, return_tensors="pt").to(device)

outputs = model.generate(**inputs, max_new_tokens=300)
print(tokenizer.decode(outputs[0], skip_special_tokens=True))
```

å¦‚ä¸‹æ˜¯ä½¿ç”¨Chinese-Mistral-7B-Instructè¿›è¡Œæ¨ç†çš„ä»£ç ç¤ºä¾‹ã€‚
```python
import torch
from transformers import AutoTokenizer, AutoModelForCausalLM

device = torch.device("cuda:0") if torch.cuda.is_available() else torch.device("cpu")

model_path = "itpossible/Chinese-Mistral-7B-Instruct-v0.2"
tokenizer = AutoTokenizer.from_pretrained(model_path)
model = AutoModelForCausalLM.from_pretrained(model_path, torch_dtype=torch.bfloat16, device_map=device)

text = "è¯·ä¸ºæˆ‘æ¨èä¸­å›½ä¸‰åº§æ¯”è¾ƒè‘—åçš„å±±"
messages = [{"role": "user", "content": text}]

inputs = tokenizer.apply_chat_template(messages, return_tensors="pt").to(device)
outputs_id = model.generate(inputs, max_new_tokens=300, do_sample=True)
outputs = tokenizer.batch_decode(outputs_id, skip_special_tokens=True)[0]
print(outputs)
```
Chinese-Mistral-7B ä¸Chinese-Mistral-7B-Instructç”Ÿæˆæ•ˆæœç¤ºä¾‹å¦‚ä¸‹ï¼š

|     æ¨¡å‹åç§°    	|      Chinese-Mistral-7B    	|     Chinese-Mistral-7B-Instruct    	|
|---	|---	|---	|
|     ç”¨æˆ·è¾“å…¥    	|     <div style="width: 270pt">æˆ‘æ˜¯ä¸€ä¸ªäººå·¥æ™ºèƒ½åŠ©æ‰‹ï¼Œæˆ‘èƒ½å¤Ÿå¸®åŠ©ä½ åšå¦‚ä¸‹è¿™äº›äº‹æƒ…ï¼š    	|     <div style="width: 270pt">è¯·ä¸ºæˆ‘æ¨èä¸­å›½ä¸‰åº§æ¯”è¾ƒè‘—åçš„å±±    	|
|     æ¨¡å‹è¾“å‡º    	|     <div style="width: 270pt">1ã€æœç´¢èµ„æ–™ï¼Œæˆ‘å¯ä»¥å¸®ä½ æœç´¢ä»»ä½•ä½ éœ€è¦çš„èµ„æ–™ã€‚æ¯”å¦‚æœç´¢æŸæœ¬ä¹¦çš„å†…å®¹ï¼Œæœç´¢æŸæŸåœ°çš„åœ°å›¾ï¼Œæœç´¢æŸéƒ¨ç”µå½±çš„æ’­æ”¾åœ°å€ç­‰ã€‚<br>2ã€è®¡ç®—ï¼Œæˆ‘å¯ä»¥å¸®ä½ è®¡ç®—å„ç§æ•°å­¦é¢˜ï¼Œæ¯”å¦‚åŠ æ³•ï¼Œå‡æ³•ï¼Œä¹˜ç§¯ï¼Œé™¤æ³•ç­‰ç­‰ã€‚è®¡ç®—å„ç§ä¸åŒç§‘ç›®çš„å…¬å¼ï¼Œæ¯”å¦‚åŒ–å­¦æ–¹ç¨‹å¼çš„è®¡ç®—ï¼Œç‰©ç†å…¬å¼çš„è®¡ç®—ç­‰ã€‚<br>3ã€åšç¬”è®°ï¼Œä½ å¯ä»¥æŠŠä½ çš„æƒ³æ³•è®°å½•åœ¨æˆ‘çš„å­—å…¸é‡Œé¢ï¼Œæˆ–è€…è®°å½•æ—¥ç¨‹å®‰æ’ã€‚<br>4ã€ç¿»è¯‘ï¼Œä½ å¯ä»¥æŠŠçš„è®¾ç½®ä¸ºä½ çš„å³æ—¶ç¿»è¯‘å™¨ï¼Œå°†ä½ è¦ç¿»è¯‘çš„å†…å®¹æ”¾åˆ°ä¿¡æ¯æ¡†é‡Œé¢ï¼Œç„¶åç‚¹å‡»å‘é€å°±å¯ä»¥ã€‚    	|     <div style="width: 270pt">ä¸­å›½çš„å±±å·é£æ™¯ååˆ†ç¾ä¸½ï¼Œä»¥ä¸‹ä¸‰åº§å±±æ˜¯å›½å†…æ¯”è¾ƒè‘—åçš„æ™¯ç‚¹ï¼š<br>1.æ³°å±±ï¼šæ³°å±±ï¼Œä½äºå±±ä¸œçœæ³°å®‰å¸‚åŒ—éƒ¨ï¼Œå†å²æ‚ ä¹…ï¼Œæ˜¯æˆ‘å›½äº”å¤§åå±±ä¹‹ä¸€ï¼Œæµ·æ‹”çº¦1545ç±³ï¼Œå…¶é›„ä¼Ÿçš„åœ°åŠ¿å’Œå£®ä¸½çš„é£å…‰å¸å¼•äº†ä¸å°‘æ¸¸å®¢å‰æ¥æ¸¸è§ˆã€‚<br>2.é»„å±±ï¼šé»„å±±ä½äºå®‰å¾½çœä¸œå—éƒ¨ï¼Œå› ç‹¬ç‰¹çš„å±±æ°´é£å…‰å’Œä¸°å¯Œçš„æ–‡åŒ–å’Œå†å²ç§¯æ·€è€Œé—»åäºä¸–ï¼Œè¿™é‡Œæ‚¬å´–å³­å£ï¼Œå¥‡å³°æ€ªçŸ³ï¼Œäº‘æµ·é›¾æµ·ï¼Œæ™¯è‰²å¥‡ç‰¹ç§€ä¸½ï¼Œè¢«èª‰ä¸ºâ€œå¤©ä¸‹ç¬¬ä¸€å¥‡å±±â€ã€‚<br>3.å³¨çœ‰å±±ï¼šå³¨çœ‰å±±ä½äºå››å·çœå³¨çœ‰å±±å¸‚ä¸œåŒ—éƒ¨ï¼Œæ˜¯ä¸­å›½å››å¤§ä½›æ•™åå±±ä¹‹ä¸€ï¼Œå› é›„ä¼Ÿå£®è§‚çš„å±±å³°å’Œä¸°å¯Œå¤šå½©çš„æ£®æ—èµ„æºè€Œé—»åäºä¸–ã€‚è¿™é‡Œæ°”å€™æ¹¿æ¶¦ï¼Œæ¤è¢«è¦†ç›–ç‡æé«˜ï¼Œæ™¯è‰²ç§€ä¸½ï¼Œè¢«èµèª‰ä¸ºâ€œå³¨çœ‰å¤©ä¸‹ç§€â€ã€‚    	|

## ğŸ“ˆ æ¨¡å‹æ€§èƒ½

### æ¨¡å‹ç»¼åˆèƒ½åŠ›

æˆ‘ä»¬é‡‡ç”¨C-Evalã€CMMLUå’ŒMMLUä¸‰ä¸ªè¯„æµ‹æ•°æ®é›†å…¨é¢è¯„ä¼°Chinese-Mistral-7Bï¼š

- C-Evalï¼šå®ƒæ˜¯ä¸€ä¸ªå…¨é¢çš„ä¸­æ–‡åŸºç¡€æ¨¡å‹è¯„ä¼°å¥—ä»¶ã€‚åŒ…å«13948ä¸ªå¤šé¡¹é€‰æ‹©é¢˜ï¼Œæ¶µç›–52ä¸ªå­¦ç§‘å’Œå››ä¸ªéš¾åº¦çº§åˆ«ã€‚å®ƒæ—¨åœ¨è¯„ä¼°æ¨¡å‹åœ¨äººæ–‡ã€ç¤¾ç§‘ã€ç†å·¥ç­‰å¤šä¸ªå­¦ç§‘å¤§ç±»ä¸Šçš„çŸ¥è¯†å’Œæ¨ç†èƒ½åŠ›ã€‚
- CMMLUï¼šå®ƒæ˜¯ä¸€ä¸ªç»¼åˆæ€§çš„ä¸­æ–‡è¯„ä¼°åŸºå‡†ã€‚æ¶µç›–äº†ä»åŸºç¡€å­¦ç§‘åˆ°é«˜çº§ä¸“ä¸šæ°´å¹³çš„67ä¸ªä¸»é¢˜ã€‚å®ƒä¸“é—¨ç”¨äºè¯„ä¼°è¯­è¨€æ¨¡å‹åœ¨ä¸­æ–‡è¯­å¢ƒä¸‹çš„çŸ¥è¯†å’Œæ¨ç†èƒ½åŠ›ã€‚
- MMLUï¼šå®ƒæ˜¯ä¸€ä¸ªåŒ…å«äº†57ä¸ªå­ä»»åŠ¡çš„è‹±æ–‡è¯„æµ‹æ•°æ®é›†ã€‚æ¶µç›–äº†ä»åˆç­‰æ•°å­¦ã€ç¾å›½å†å²ã€è®¡ç®—æœºç§‘å­¦åˆ°æ³•å¾‹ç­‰å¤šä¸ªé¢†åŸŸï¼Œéš¾åº¦è¦†ç›–é«˜ä¸­æ°´å¹³åˆ°ä¸“å®¶æ°´å¹³ï¼Œæœ‰æ•ˆåœ°è¡¡é‡äº†æ¨¡å‹åœ¨äººæ–‡ã€ç¤¾ç§‘å’Œç†å·¥ç­‰å¤šä¸ªå­¦ç§‘å¤§ç±»ä¸­çš„ç»¼åˆçŸ¥è¯†èƒ½åŠ›ã€‚

ä¸‹è¡¨å±•ç¤ºäº†å¼€æºç¤¾åŒºè¾ƒæµè¡Œçš„ä¸­æ–‡Llama2ã€ä¸­æ–‡Mistralä¸æˆ‘ä»¬å‘å¸ƒçš„Chinese-Mistral-7Bçš„è¯„æµ‹ç»“æœã€‚è¯„æµ‹æ–¹å¼é‡‡ç”¨5-shotï¼Œé‡‡ç”¨opencompassåœ¨ç›¸åŒçš„å®éªŒæ¡ä»¶ä¸‹è¿›è¡Œè¯„æµ‹ã€‚

|                                              æ¨¡å‹åç§°                                              |    C-Eval     |      CMMLU    |    MMLU      |    å¹³å‡å¾—åˆ†        |
|:-----------------------------------------------------------------------------------------------:|:-------------:|:-------------:|:------------:|:-----------------:|
|    [Linly-Al/Chinese-LLaMA-2-7B-hf](https://huggingface.co/Linly-Al/Chinese-LLaMA-2-7B-hf)      |     31.2      |     30.14     |    35.09     |       32.14       |
|             [hfl/chinese-llama-2-7b](https://huggingface.co/hfl/chinese-llama-2-7b)             |     27.4      |     33.38     |    37.25     |       32.68       |
|    [Linly-Al/Chinese-LLaMA-2-13B-hf](https://huggingface.co/Linly-Al/Chinese-LLaMA-2-13B-hf)    |     39.9      |     42.48     |    52.54     |       44.97       |
|            [hfl/chinese-llama-2-13b](https://huggingface.co/hfl/chinese-llama-2-13b)            |     41.0      |     43.25     |    52.94     |       45.73       |
|      [gywy/Mistral-7B-v0.1-chinese](https://huggingface.co/gywy/Mistral-7B-v0.1-chinese)        |     37.4      |     36.45     |    37.38     |       37.08       |
|[OpenBuddy/openbuddy-mistral-7b-v13-base](https://huggingface.co/OpenBuddy/openbuddy-mistral-7b-v13-base)|     44.4      |     46.32     |    57.79     |       49.50       |
|                                  **[Chinese-Mistral-7B (æœ¬æ¨¡å‹)](https://huggingface.co/itpossible/Chinese-Mistral-7B-v0.1)**                                  |     **47.5**      |     **47.52**     |    **58.29**     |       **51.10**       |

ç”±ä¸Šè¡¨å¯çŸ¥ï¼ŒChinese-Mistral-7Bçš„ä¸­æ–‡å’Œè‹±æ–‡é€šè¯†èƒ½åŠ›ä¸ä»…è¶…è¿‡åŒç­‰å‚æ•°é‡çš„ä¸­æ–‡Llama2æ¨¡å‹ï¼Œè€Œä¸”åœ¨å¤šé¡¹è¯„æµ‹ä¸­ä¼˜äº130äº¿å‚æ•°é‡çš„ä¸­æ–‡Llama2ã€‚åŒæ—¶ï¼ŒChinese-Mistral-7Bçš„è¯„æµ‹è¡¨ç°é«˜äºå¼€æºç¤¾åŒºå…¶ä»–åŒç­‰å‚æ•°é‡çš„ä¸­æ–‡Mistralã€‚

### ä¸­æ–‡ç¼–è§£ç æ•ˆç‡

æˆ‘ä»¬ä»WuDaoCorpus2ä¸­é‡‡æ ·è®­ç»ƒæ•°æ®ï¼Œä½¿ç”¨sentencepieceè®­ç»ƒä¸­æ–‡BPEè¯è¡¨ï¼Œå¹¶äººå·¥é€‰å–éƒ¨åˆ†å…¶ä»–ä¼˜ç§€ä¸­æ–‡è¯è¡¨è¿›è¡Œè¯è¡¨èåˆã€‚ç»è¿‡ä¸¥æ ¼çš„äººå·¥å®¡æ ¸ï¼Œæœ€ç»ˆå½¢æˆçš„è¯è¡¨å¤§å°ä¸º63776ã€‚ä¸ºäº†æé«˜æ¨¡å‹è®¡ç®—æ•ˆç‡ï¼Œæˆ‘ä»¬åœ¨è¯è¡¨æœ«å°¾æ·»åŠ <|sym1|>ã€â€¦â€¦ã€<|sym96|>ï¼Œä½¿å¾—è¯è¡¨å¤§å°ä¸º128çš„å€æ•°ï¼Œæœ€ç»ˆå¾—åˆ°çš„è¯è¡¨å¤§å°ä¸º63872ã€‚<br>
æˆ‘ä»¬éšæœºé€‰å–äº†WuDaoCorpus2_part-2021278643ä½œä¸ºæµ‹è¯•æ•°æ®ä»¥è¯„æµ‹åˆ†è¯æ•ˆæœã€‚ç»ç»Ÿè®¡ï¼Œæµ‹è¯•æ•°æ®åŒ…æ‹¬67013857ä¸ªå•è¯ï¼Œæˆ‘ä»¬ç”¨å•è¯æ•°é‡é™¤ä»¥åˆ†è¯åçš„Tokenæ•°é‡ï¼Œè®¡ç®—å‹ç¼©ç‡ã€‚å‹ç¼©ç‡è¶Šå¤§ï¼Œè¡¨æ˜åˆ†è¯æ•ˆæœè¶Šå¥½ï¼Œåœ¨ä¸­æ–‡åœºæ™¯çš„ç¼–è§£ç æ•ˆç‡è¶Šé«˜ã€‚

|                                              æ¨¡å‹åç§°                                              |    æ¨¡å‹ç±»å‹     |      è¯è¡¨å¤§å°    |    Tokenæ•°é‡      |    å‹ç¼©ç‡        |
|:-----------------------------------------------------------------------------------------------:|:-------------:|:-------------:|:------------:|:-----------------:|
|    [meta-llama/Llama-2-7b-hf](https://huggingface.co/meta-llama/Llama-2-7b-hf)      |     Llama      |     32000     |    97406876     |       0.6880       |
|             [mistralai/Mistral-7B-v0.1](https://huggingface.co/mistralai/Mistral-7B-v0.1)             |     Mistral      |     32000     |    76269008     |       0.8787       |
|             [THUDM/chatglm2-6b](https://huggingface.co/THUDM/chatglm2-6b)             |     GLM      |     64789     |    43487673     |       1.5410       |
|    [Linly-Al/Chinese-LLaMA-2-13B-hf](https://huggingface.co/Linly-Al/Chinese-LLaMA-2-13B-hf)    |     Llama      |     40076     |    65402900     |       1.0246       |
|            [hfl/chinese-llama-2-13b](https://huggingface.co/hfl/chinese-llama-2-13b)            |     Llama      |     55296     |    45763513     |       1.4644       |
|      [OpenBuddy/openbuddy-mistral-7b-v13-base](https://huggingface.co/OpenBuddy/openbuddy-mistral-7b-v13-base)        |     Mistral      |     36608     |    65329642     |       1.0256       |
|[gywy/Mistral-7B-v0.1-chinese](https://huggingface.co/gywy/Mistral-7B-v0.1-chinese)|     Mistral      |     48593     |    46670146     |       1.4359       |
|                                  **[Chinese-Mistral-7B (æœ¬æ¨¡å‹)](https://huggingface.co/itpossible/Chinese-Mistral-7B-v0.1)**                                   |     Mistral      |     63872     |    **43044156**     |       **1.5569**       |



ç”±ä¸Šè¡¨å¯çŸ¥ï¼ŒChinese-Mistral-7Båœ¨å¯è§‚çš„è¯è¡¨å¤§å°æƒ…å†µä¸‹ï¼Œå–å¾—äº†æœ€é«˜çš„å‹ç¼©ç‡ï¼Œè¡¨æ˜å…¶èƒ½å¤Ÿé«˜æ•ˆå¤„ç†ä¸­æ–‡æ–‡æœ¬ã€‚

## ğŸ“ è®­ç»ƒæ•°æ®

è®­ç»ƒæ•°æ®é‡‡æ ·äºWanJuanã€baike2018qaã€Dolmaã€gutenberg-booksç­‰é«˜è´¨é‡å¼€æºæ•°æ®é›†ã€‚æˆ‘ä»¬å¯¹è¿™äº›æ•°æ®é›†è¿›è¡Œç»†ç²’åº¦æ¸…æ´—ï¼Œå¹¶å……åˆ†è€ƒè™‘è®­ç»ƒæ•°æ®é›†ä¸­ä¸åŒç±»åˆ«æ•°æ®çš„å æ¯”ã€‚

## âš ï¸ å±€é™æ€§

Chinese-Mistral-7Bçš„å¼€å‘æ—¨åœ¨ä¸ºå¼€æºç¤¾åŒºæä¾›ä¸€ä¸ªæ€§èƒ½ä¼˜è¶Šçš„ä¸­æ–‡å¤§è¯­è¨€æ¨¡å‹ã€‚è¯·æ³¨æ„ï¼Œç”±äºæ¨¡å‹å¤§å°åŠè®­ç»ƒæ•°æ®è§„æ¨¡é™åˆ¶ï¼Œæœ¬æ¨¡å‹ä»å¯èƒ½ç”Ÿæˆè¯¯å¯¼æ€§å†…å®¹æˆ–è€…æœ‰å®³å†…å®¹ã€‚å› æ­¤ï¼Œåœ¨éƒ¨ç½²ä»»ä½•ç”±Chinese-Mistralç³»åˆ—æ¨¡å‹é©±åŠ¨çš„åº”ç”¨ç¨‹åºä¹‹å‰ï¼Œå¼€å‘äººå‘˜å¿…é¡»è¿›è¡Œå®‰å…¨æµ‹è¯•ï¼Œå¯¹æ¨¡å‹è¿›è¡Œç›¸åº”è°ƒæ•´ï¼Œä»¥æ»¡è¶³å®‰å…¨æ€§éœ€æ±‚ã€‚

## âœ’ï¸ å¼•ç”¨

å¦‚æœæ‚¨è§‰å¾—æœ¬é¡¹ç›®å¯¹æ‚¨çš„ç ”ç©¶æœ‰æ‰€å¸®åŠ©æˆ–ä½¿ç”¨äº†æœ¬é¡¹ç›®çš„æ¨¡å‹ï¼Œè¯·å¼•ç”¨æœ¬é¡¹ç›®ï¼š

```bibtex
@article{chen2024preparedllm,
  author = {Chen, Zhou and Lin, Ming and Wang, Zimeng and Zang, Mingrun and Bai, Yuqi},
  title = {PreparedLLM: Effective Pre-pretraining Framework for Domain-specific Large Language Models},
  year = {2024},
  journal = {Big Earth Data},
  pages = {1--24},
  doi = {10.1080/20964471.2024.2396159},
  url = {https://doi.org/10.1080/20964471.2024.2396159}
}

@misc{Chinese-Mistral,
    author = {Zhou, Chen and Yuqi, Bai},
    title = {Chinese-Mistral: An Efficient and Effective Chinese Large Language Model},
    year = {2024},
    publisher = {GitHub},
    journal = {GitHub repository},
    howpublished = {\url{https://github.com/THU-ESIS/Chinese-Mistral}}
}
```

## ç»“è¯­
æˆ‘ä»¬æ¬¢è¿ç¤¾åŒºçš„æ”¯æŒå’Œåˆä½œï¼Œå…±åŒæ¨åŠ¨é€šç”¨å¤§è¯­è¨€æ¨¡å‹å’Œé¢†åŸŸå¤§è¯­è¨€æ¨¡å‹çš„å‘å±•ã€‚è”ç³»æ–¹å¼ï¼š<br>
ç™½ç‰çªï¼Œæ¸…åå¤§å­¦åœ°çƒç³»ç»Ÿç§‘å­¦ç³»é•¿è˜æ•™æˆï¼Œå®éªŒå®¤è´Ÿè´£äººï¼Œyuqibai@tsinghua.edu.cn<br>
é™ˆèˆŸï¼Œæ¸…åå¤§å­¦åœ°çƒç³»ç»Ÿç§‘å­¦ç³»åšå£«ç”Ÿï¼Œå¤§è¯­è¨€æ¨¡å‹ç»„ç»„é•¿ï¼Œchenz22@mails.tsinghua.edu.cn
